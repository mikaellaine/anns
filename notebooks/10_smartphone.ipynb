{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data available on Kaggle for smartphone accelerometer data labelled by user action: <br>https://www.kaggle.com/uciml/human-activity-recognition-with-smartphones/data\n",
    "<br>This type of data is a great example of what we can feed a neural network: obscure data in abundance (561 values per datapoint) that correlates in some way with a classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import sys, re\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"examplemodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "po = pd.read_csv(\"./smartphone/train.csv\")\n",
    "print(po.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have a lot of data to look at. We're not going to try to understand the data ourselves, but rather let a deep learning model understand it for us.\n",
    "<br>First, lets load the data and separate it into labels and input. As you can see from the above, column index 1 contains the activity labels and the rest of each row (beginning from index 2) contains the sensors data from the smartphone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "po = pd.read_csv(\"./smartphone/train.csv\")\n",
    "sensors = po[po.columns[2:]]\n",
    "actions = po[po.columns[1]]\n",
    "print(str(sensors.shape))\n",
    "print(str(actions.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the neural network model, the labels need to be integers. The loss function tf.losses.sparse_softmax_cross_entropy requires int32 or int64.<br>This can be achieved by using the index to a list of activity strings. Lets create that list and make a note of the corresponding index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_labels = []\n",
    "action_indices = []\n",
    "\n",
    "#Go through all action strings and store the unique strings into action_labels\n",
    "print(\"Collecting all the action labels: \")\n",
    "for s in range(0,len(actions)):\n",
    "    lbln = actions[s]\n",
    "    if lbln not in action_labels:\n",
    "        action_labels.append(lbln)\n",
    "        print(lbln)\n",
    "    action_indices.append(action_labels.index(lbln))\n",
    "    \n",
    "#Convert into numpy arrays for tensorflow\n",
    "input = np.asarray(sensors, np.float32)\n",
    "output = np.asarray(action_indices, np.int32)\n",
    "\n",
    "print(\"Input/output pairs prepared: \")\n",
    "print(\"Input: \"+str(input.shape))\n",
    "print(\"Output: \"+str(output.shape))\n",
    "print(\"The first 20 labels look like this now: \"+str(output[0:20]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 3609 data samples, we can think of our training strategy: how much of the data to use for training, and how much for evaluation. Remember, the model may become overfitted for the training data and not be general enough to correctly predict new data. <br>We will reserve about 27% of the data for evaluation later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_input  =  input[0:1000,:]\n",
    "eval_output = output[0:1000]\n",
    "input  =  input[1000:,:]\n",
    "output = output[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On to the exciting stuff! Next, we'll define the model function. Add hidden layers as follows:\n",
    "<li>2d convolutional layer\n",
    "<li>max pooling layer with strides and pool size 2\n",
    "<li>2d convolutional layer\n",
    "<li>Dense layer with CLASSESCNT nodes (this will be the output)\n",
    "<p>If you like, you can also use 1d layers, such as tf.layers.conv1d and tf.layers.max_pooling1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSESCNT = len(action_labels)\n",
    "DATALEN = input.shape[1]#number of values per sample (second dimension of the shape)\n",
    "def model_function(features, labels, mode):\n",
    "  global DATALEN\n",
    "  global CLASSESCNT\n",
    "  inpu = tf.reshape(features[\"x\"], [-1, DATALEN, 1, 1])\n",
    "  \n",
    "  logits = tf.reshape(dense, [-1, CLASSESCNT])\n",
    "\n",
    "  predictions = {\n",
    "    \"classes\": tf.argmax(input=logits, axis=1),\n",
    "    \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "  }\n",
    "\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "  loss = tf.losses.sparse_softmax_cross_entropy( labels = labels, logits=logits )\n",
    "  tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "      optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "      gradients, variables = zip(*optimizer.compute_gradients(loss))\n",
    "      gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "      optimize = optimizer.apply_gradients(zip(gradients, variables), global_step=tf.train.get_global_step())\n",
    "      return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=optimize)\n",
    "\n",
    "  eval_metric_ops = { \"accuracy\": tf.metrics.accuracy(labels=labels, predictions=predictions[\"classes\"])}\n",
    "  return tf.estimator.EstimatorSpec( mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_checkpointing_config = tf.estimator.RunConfig(\n",
    "    save_checkpoints_secs = 60*1,\n",
    "    keep_checkpoint_max = 10,\n",
    ")\n",
    "estimator = tf.estimator.Estimator(model_fn=model_function, model_dir=\"examplemodel\", config=my_checkpointing_config)\n",
    "\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": input},\n",
    "    y=output,\n",
    "    batch_size=100,\n",
    "    num_epochs=None,\n",
    "    shuffle=True)\n",
    "\n",
    "estimator.train( input_fn=train_input_fn, steps = 10000 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Eval\")\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn( x={\"x\":eval_input}, y=eval_output, num_epochs=1, shuffle=False )\n",
    "eval_results = estimator.evaluate(input_fn=eval_input_fn)\n",
    "#fin = eval_results.__next__()\n",
    "print(\"result size: \"+str(list(eval_results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
